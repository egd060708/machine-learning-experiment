{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "695978f0",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (413212976.py, line 141)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[8], line 141\u001b[0;36m\u001b[0m\n\u001b[0;31m    self.loss_valid[i] = hinge_loss(self..x_valid, self.y_valid, self.theta, C)\u001b[0m\n\u001b[0m                                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.datasets as sd\n",
    "import sklearn.model_selection as sms\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "# 读取实验训练集和验证集\n",
    "X_train, y_train = sd.load_svmlight_file('../data/a9a.txt',n_features = 123)\n",
    "X_valid, y_valid = sd.load_svmlight_file('../data/a9a.t.txt',n_features = 123)\n",
    "\n",
    "\n",
    "# 将稀疏矩阵转为ndarray类型\n",
    "X_train = X_train.toarray()#转化为矩阵\n",
    "X_valid = X_valid.toarray()\n",
    "y_train = y_train.reshape(len(y_train),1)#转为列向量\n",
    "y_valid = y_valid.reshape(len(y_valid),1)\n",
    "\n",
    "print(X_train.shape)\n",
    "\n",
    "#X_train.shape[0]返回数组的大小在数据前面多加一列1\n",
    "X_train = np.concatenate((np.ones((X_train.shape[0],1)), X_train), axis = 1)#给x矩阵多增加1列1，拼接在每一行的末尾\n",
    "X_valid = np.concatenate((np.ones((X_valid.shape[0],1)), X_valid), axis = 1)\n",
    "\n",
    "#定义sigmoid函数，传入参数为向量时，对向量的每个参数进行运算后返回一个列表\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "#定义 logistic loss 函数\n",
    "def logistic_loss(X, y ,theta):\n",
    "    hx = sigmoid(X.dot(theta))\n",
    "    cost = np.multiply((1+y), np.log(1+hx)) + np.multiply((1-y), np.log(1-hx))\n",
    "    return -cost.mean()/2\n",
    "\n",
    "#定义 logistic gradient 函数\n",
    "def logistic_gradient(X, y, theta):\n",
    "    return X.T.dot(sigmoid(X.dot(theta)) - y)\n",
    "\n",
    "#定义 hinge loss 函数\n",
    "def hinge_loss(X, y, theta, t):\n",
    "    loss = np.maximum(0, 1 - np.multiply(y, X.dot(theta))).mean()\n",
    "    reg = np.multiply(theta,theta).sum() / 2\n",
    "    return t * loss + reg\n",
    "\n",
    "#定义 hinge gradient 函数\n",
    "def hinge_gradient(X, y, theta, C):\n",
    "    error = np.maximum(0, 1 - np.multiply(y, X.dot(theta)))\n",
    "    index = np.where(error==0)\n",
    "    x = X.copy()\n",
    "    x[index,:] = 0\n",
    "    grad = theta - C * x.T.dot(y) / len(y)\n",
    "    grad[-1] = grad[-1] - theta[-1]\n",
    "    return grad\n",
    "\n",
    "\n",
    "\n",
    "# 定义梯度下降框架\n",
    "class Regression_frame:\n",
    "    def __init__(self,x_train,x_valid,y_train,y_valid,loss_func,gradient):\n",
    "        self.x_train = x_train\n",
    "        self.x_valid = x_valid\n",
    "        self.y_train = y_train\n",
    "        self.y_valid = y_valid\n",
    "        self.loss_func = loss_func\n",
    "        self.gradient = gradient\n",
    "        self.theta = 0 # 初始化\n",
    "        self.loss_train = []\n",
    "        self.loss_valid = []\n",
    "        \n",
    "    # 线性模型参数初始化\n",
    "    def init_params(self, mode=\"zero\"):\n",
    "        if mode == \"zero\": # 零初始化\n",
    "            self.theta = np.zeros((self.x_train.shape[1],1))\n",
    "        elif mode == \"normal\": # 正态分布初始化\n",
    "            self.theta = np.random.normal(0,1,(self.x_train.shape[1],1))\n",
    "        elif mode == \"random\": # 随机初始化\n",
    "            self.theta = np.random.rand(self.x_train.shape[1],1)\n",
    "                                  \n",
    "    #线性回归随机梯度下降\n",
    "    def linear_random_descent(self, alpha, iters):\n",
    "        n=self.x_train.shape\n",
    "        self.loss_train = np.zeros((iters,1))\n",
    "        self.loss_valid = np.zeros((iters,1))\n",
    "        for i in range(iters):\n",
    "            #随机选择一个样本\n",
    "            num=np.random.randint(n,size=1)\n",
    "            x_select=self.x_train[num,:]\n",
    "            y_select=self.y_train[num,0]\n",
    "            grad = self.gradient(x_select, y_select, self.theta)\n",
    "            self.theta = self.theta - alpha * grad\n",
    "            self.loss_train[i] = self.loss_func(self.x_train, self.y_train, self.theta)\n",
    "            self.loss_valid[i] = self.loss_func(self.x_valid, self.y_valid, self.theta)\n",
    "        return self.theta, self.loss_train, self.loss_valid\n",
    "\n",
    "    #线性回归全梯度下降\n",
    "    def linear_descent(self, alpha, iters):\n",
    "        self.loss_train = np.zeros((iters,1))\n",
    "        self.loss_valid = np.zeros((iters,1))\n",
    "        for i in range(iters):\n",
    "            grad = self.gradient(self.x_train, self.y_train, self.theta)\n",
    "            self.theta = self.theta - alpha * grad\n",
    "            self.loss_train[i] = self.loss_func(self.x_train, self.y_train, self.theta)\n",
    "            self.loss_valid[i] = self.loss_func(self.x_valid, self.y_valid, self.theta)\n",
    "        return self.theta, self.loss_train, self.loss_valid\n",
    "    \n",
    "    #逻辑回归小批量梯度下降\n",
    "    def logistic_descent(self, alpha, iters, batch_size):\n",
    "        self.loss_train = np.zeros((iters,1))\n",
    "        self.loss_valid = np.zeros((iters,1))\n",
    "        data = np.concatenate((self.y_train, self.x_train), axis=1)\n",
    "        for i in range(iters):\n",
    "            sample = np.matrix(random.sample(data.tolist(), batch_size))\n",
    "            grad = self.gradient(sample[:,1:125], sample[:,0], self.theta)#梯度\n",
    "            self.theta = self.theta - alpha * grad#更新参数模型\n",
    "            self.loss_train[i] = self.loss_func(self.x_train, self.y_train, self.theta)\n",
    "            self.loss_valid[i] = self.loss_func(self.x_train, self.y_train, self.theta)\n",
    "        return self.theta, self.loss_train, self.loss_valid\n",
    "    \n",
    "    #定义sigmoid函数，传入参数为向量时，对向量的每个参数进行运算后返回一个列表\n",
    "    def sigmoid(self,z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    #定义 logistic score 函数，设置阈值为0.5\n",
    "    def logistic_score(self):\n",
    "        hx = self.sigmoid(self.x_valid.dot(self.theta))\n",
    "        hx[hx>=0.5] = 1\n",
    "        hx[hx<0.5] = -1\n",
    "        hx = (hx==self.y_valid)\n",
    "        return np.mean(hx)\n",
    "    \n",
    "    #定义 svm decent 函数\n",
    "    def svm_descent(self, alpha, iters, batch_size, C):\n",
    "        self.loss_train = np.zeros((iters,1))\n",
    "        self.loss_valid = np.zeros((iters,1))\n",
    "        data = np.concatenate((self.y_train, self.x_train), axis=1)\n",
    "        for i in range(iters):\n",
    "            sample = np.matrix(random.sample(data.tolist(), batch_size))\n",
    "            grad = self.gradient(sample[:,1:125], sample[:,0], self.theta, C)\n",
    "            self.theta = self.theta - alpha * grad\n",
    "            self.loss_train[i] = self.loss_func(self.x_train, self.y_train, self.theta, C)\n",
    "            self.loss_valid[i] = self.loss_func(self..x_valid, self.y_valid, self.theta, C)\n",
    "        return self.theta, self.loss_train, self.loss_valid\n",
    "    \n",
    "    #定义 svm score 函数\n",
    "    def svm_score(self):\n",
    "        hx = self.x_valid.dot(self.theta)\n",
    "        hx[hx>=5] = 1\n",
    "        hx[hx<5] = -1\n",
    "        hx = (hx==self.y_valid)\n",
    "        return np.mean(hx)\n",
    "    \n",
    "    # 输出最小loss并且画图\n",
    "    def getLoss_and_plot(self, iters):\n",
    "        print(self.loss_train.min())\n",
    "        print(self.loss_valid.min())\n",
    "        iteration = np.arange(0, iters, step = 1)\n",
    "        fig, ax = plt.subplots(figsize = (12,8))\n",
    "        ax.set_title('Train')\n",
    "        ax.set_xlabel('iteration')\n",
    "        ax.set_ylabel('loss')\n",
    "        plt.plot(iteration, self.loss_train, 'b', label='Train')\n",
    "        plt.plot(iteration, self.loss_valid, 'r', label='Valid')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "# question1\n",
    "logistic = Regression_frame(X_train, X_valid, y_train, y_valid, logistic_loss, logistic_gradient)\n",
    "alpha = 0.0001\n",
    "num_iters = 150\n",
    "batch_size = 100\n",
    "logistic.init_params(mode = \"zero\")\n",
    "logistic.logistic_descent(alpha, num_iters, batch_size)\n",
    "logistic.getLoss_and_plot(num_iters)\n",
    "print(logistic.logistic_score())\n",
    "\n",
    "# question2\n",
    "svm = Regression_frame(X_train, X_valid, y_train, y_valid, hinge_loss, hinge_gradient)\n",
    "alpha = 0.01\n",
    "num_iters = 250\n",
    "batch_size = 100\n",
    "C = 0.5\n",
    "svm.init_params(mode = \"zero\")\n",
    "svm.logistic_descent(alpha, num_iters, batch_size, C)\n",
    "svm.getLoss_and_plot(num_iters)\n",
    "print(svm.svm_score())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
